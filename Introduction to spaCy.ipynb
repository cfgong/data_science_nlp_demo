{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to [spaCy](https://spacy.io/)\n",
    "*Industrial-Strength Natural Language Processing in Python*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spaCy is a Python libary for NLP\n",
    "- supports multiple languages, staistical models\n",
    "- provides support for tokenization, word vectors, tagging, parsing, segmentation, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Resources:\n",
    "- [spacy101](https://spacy.io/usage/spacy-101) \n",
    "- [Introduction to NLP with spaCy](https://towardsdatascience.com/a-short-introduction-to-nlp-in-python-with-spacy-d0aa819af3ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install, go to terminal and run \n",
    "```\n",
    "pip install -U spacy\n",
    "```\n",
    "After installation, also need to download the language model \n",
    "```\n",
    "python -m spacy download en_core_web_lg\n",
    "```\n",
    "\n",
    "To use spacy with English:\n",
    "```\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install spacy for jupyter notebook.\n",
    "try:\n",
    "    from pip import main as pipmain\n",
    "except:\n",
    "    from pip._internal import main as pipmain\n",
    "packages = ['spacy']\n",
    "pipmain(['install'] + packages);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
      "\u001b[K     |████████████████████████████████| 827.9MB 10.9MB/s eta 0:00:01  |▏                               | 3.0MB 1.8MB/s eta 0:07:36     |▉                               | 21.1MB 9.8MB/s eta 0:01:23     |██████                          | 153.9MB 7.5MB/s eta 0:01:30     |████████▍                       | 216.4MB 11.6MB/s eta 0:00:53     |█████████▎                      | 240.1MB 3.4MB/s eta 0:02:55     |████████████▏                   | 314.9MB 11.2MB/s eta 0:00:46     |████████████▋                   | 327.6MB 8.3MB/s eta 0:01:01     |██████████████▏                 | 365.1MB 11.9MB/s eta 0:00:39     |██████████████▏                 | 367.1MB 11.9MB/s eta 0:00:39     |██████████████▌                 | 373.9MB 6.0MB/s eta 0:01:17     |████████████████▍               | 424.6MB 11.3MB/s eta 0:00:36     |████████████████▊               | 432.9MB 15.0MB/s eta 0:00:27     |█████████████████               | 437.2MB 7.7MB/s eta 0:00:51     |██████████████████              | 467.3MB 10.7MB/s eta 0:00:34     |██████████████████▋             | 481.5MB 12.8MB/s eta 0:00:27     |████████████████████            | 519.9MB 30.7MB/s eta 0:00:11     |████████████████████▏           | 521.9MB 30.7MB/s eta 0:00:10     |████████████████████▋           | 532.6MB 12.6MB/s eta 0:00:24     |████████████████████▉           | 540.4MB 12.0MB/s eta 0:00:2424     |██████████████████████▍         | 578.5MB 4.2MB/s eta 0:01:00        | 591.9MB 10.8MB/s eta 0:00:22     |███████████████████████         | 594.6MB 10.8MB/s eta 0:00:22     |███████████████████████         | 595.7MB 10.8MB/s eta 0:00:22     |███████████████████████▋        | 611.5MB 13.2MB/s eta 0:00:17     |████████████████████████▌       | 633.2MB 11.1MB/s eta 0:00:18     |████████████████████████▊       | 638.2MB 11.1MB/s eta 0:00:18     |████████████████████████▊       | 638.8MB 11.1MB/s eta 0:00:18     |█████████████████████████▌      | 660.5MB 7.9MB/s eta 0:00:22     |██████████████████████████▎     | 679.9MB 9.0MB/s eta 0:00:17     |███████████████████████████     | 695.6MB 4.1MB/s eta 0:00:33     |███████████████████████████▍    | 708.2MB 5.3MB/s eta 0:00:23     |██████████████████████████████▋ | 790.9MB 12.6MB/s eta 0:00:03     |███████████████████████████████▏| 806.7MB 12.7MB/s eta 0:00:02     |███████████████████████████████▏| 807.3MB 12.7MB/s eta 0:00:02     |███████████████████████████████▎| 808.5MB 12.7MB/s eta 0:00:02     |███████████████████████████████▉| 823.6MB 10.9MB/s eta 0:00:01\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: '/private/var/folders/62/n3ych4r91zz72tws59mdh_780000gn/T/pip-req-tracker-a79ikjep/2ac1a6fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc'\n",
      "\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download --user en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-5469aec04266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_lg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "- split text into words, symbols, punctuation a.k.a. tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'hungry,',\n",
       " 'hungry',\n",
       " 'catepillar',\n",
       " 'ate',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'food,',\n",
       " 'and',\n",
       " 'then',\n",
       " 'he',\n",
       " 'became',\n",
       " 'a',\n",
       " 'butterfly!']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"The hungry, hungry catepillar ate all of the food, and then he became a butterfly!\")\n",
    "doc.text.split() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the punctuation gets attached to the previous word. We don't want that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'hungry',\n",
       " ',',\n",
       " 'hungry',\n",
       " 'catepillar',\n",
       " 'ate',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'food',\n",
       " ',',\n",
       " 'and',\n",
       " 'then',\n",
       " 'he',\n",
       " 'became',\n",
       " 'a',\n",
       " 'butterfly',\n",
       " '!']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.orth_ for token in doc] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove punctuation by using `.is_punct`   \n",
    "remove spaces by using: `.is_space`   \n",
    "remove stop words by using the `.is_stop`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hungry', 'hungry', 'catepillar', 'ate', 'food', 'butterfly']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.orth_ for token in doc if not token.is_punct | token.is_space | token.is_stop] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how all the punctuation, white spaces, and stop words have been removed and we are left only with the \"important\" words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "- reducing a word to its base form or root form\n",
    "- reduce various wordforms to its citation form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use spacy's `.lemma_` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'go', 'go', 'go']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_words = \"going gone went goes\" \n",
    "nlp_lemma_words = nlp(lemma_words) \n",
    "[word.lemma_ for word in nlp_lemma_words] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is especially useful for text classification because lemmatising the text helps avoids word duplication for building models like bag of words model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts-of-speech (POS) Tagging\n",
    "- assign the to words \n",
    "- spacy uses [Penn Treebank POS tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the `.pos_` and `.tag_` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(My, 'PRP$'),\n",
       " (dog, 'NN'),\n",
       " ('s, 'POS'),\n",
       " (toy, 'NN'),\n",
       " (actually, 'RB'),\n",
       " (belongs, 'VBZ'),\n",
       " (to, 'IN'),\n",
       " (the, 'DT'),\n",
       " (neighbor, 'NN'),\n",
       " ('s, 'POS'),\n",
       " (cat, 'NN'),\n",
       " (., '.')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp(\"My dog's toy actually belongs to the neighbor's cat.\") \n",
    "pos_tags = [(i, i.tag_) for i in doc2]\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a list of owner-possesion tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(dog, toy), (neighbor, cat)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i[0].nbor(-1), i[0].nbor(+1)) for i in pos_tags if i[1] == \"POS\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors\n",
    "- the concept of word embeddings is that every word can be represented as a set of real numbers (vectors) that capture the word meaning and context\n",
    "- each word has a unique embedding\n",
    "- word embeddings are multidimensional\n",
    "- similar words have similar embedding values\n",
    "\n",
    "Resources:\n",
    "- [spacy.io: Word Vectors and Semantic Similarity](https://spacy.io/usage/vectors-similarity)\n",
    "- [Get Busy With Word Embeddings](https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/)\n",
    "- [Word Embeddings in Python with Spacy and Gensim](https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy provides pre-trained models for word embeddings which downloaded when we downloaded the English model. Spacy can parse entire blocks of text and assigns word vectors using the loaded model. Then, use `.vector` to get the word vector. \n",
    "\n",
    "Important Note: spaCy's small models (models that end in `sm`) don't ship with word vectors. You can still use `.similarity` to compares, but the results won't be as good. To use real word vectors, make sure to download the large models:\n",
    "```\n",
    "python -m spacy download en_core_web_lg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat [ 0.75340027  1.3175309  -1.7617409   0.31828502  1.638488    1.0552995\n",
      "  0.31595042  5.7819796   0.0343391   4.0019946   5.2300787   0.15606856\n",
      "  3.4243002  -2.4221869   1.6035937   1.337295   -1.2828892   1.8265244\n",
      " -1.3817635  -2.1414158  -0.18950051  0.08884555 -0.27133894 -0.47963017\n",
      " -0.2571426  -2.404962    1.0642331  -3.212206    0.37124443  1.3374927\n",
      "  0.8305371  -0.90479285  0.8499906  -2.0038836  -0.9727297  -1.1461185\n",
      "  3.1226678   0.9663619  -3.0638602   2.833336    1.0400918   1.2719803\n",
      " -1.575497   -3.35216    -0.17291349 -2.689811    0.5845911  -1.7116385\n",
      " -0.42257053 -0.7932979  -1.072156   -0.07230203  0.09205103 -0.05305272\n",
      " -2.469643    1.3820657   2.0382776   1.9970671  -0.41650915 -0.9046292\n",
      "  1.887304   -2.9841347  -0.55531263  1.2111204  -1.9578846  -2.7545862\n",
      "  1.9617162  -4.4497204   1.0900779   2.837864   -1.7545315   2.7429385\n",
      "  1.5484421  -0.05734076 -1.2939063  -0.29932067  1.5413947  -1.9647777\n",
      " -0.3654107  -0.79876906  2.2411642  -1.6222702  -3.132082   -1.3606507\n",
      "  3.0122917   1.0282891  -1.8395088  -1.4939873  -0.07344025  0.3509098\n",
      "  0.03974861 -2.9714272  -2.1116436   3.2516422   0.9638147  -0.19119883]\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u\"cat dog water cloud\")\n",
    "print(tokens[0].text, tokens[0].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the word vectors we got from spacy to compare the similarity of the words using `.similarity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog cat 0.48059145\n",
      "dog banana 0.37596428\n",
      "dog afskfsd 0.1499375\n",
      "cat dog 0.48059145\n",
      "cat cat 1.0\n",
      "cat banana 0.4528873\n",
      "cat afskfsd 0.26725474\n",
      "banana dog 0.37596428\n",
      "banana cat 0.4528873\n",
      "banana banana 1.0\n",
      "banana afskfsd 0.4848014\n",
      "afskfsd dog 0.1499375\n",
      "afskfsd cat 0.26725474\n",
      "afskfsd banana 0.4848014\n",
      "afskfsd afskfsd 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crystalgong/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/crystalgong/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/crystalgong/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/crystalgong/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/crystalgong/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/crystalgong/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/crystalgong/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/crystalgong/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/crystalgong/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/crystalgong/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/crystalgong/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/crystalgong/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Resources\n",
    "- [PythonForLinguistsTalk Gitlab](https://gitlab.com/andersonh/PythonForLinguistsTalk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
