{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to [spaCy](https://spacy.io/)\n",
    "*Industrial-Strength Natural Language Processing in Python*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spaCy is a Python libary for NLP\n",
    "- supports multiple languages, staistical models\n",
    "- provides support for tokenization, word vectors, tagging, parsing, segmentation, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Resources:\n",
    "- [spacy101](https://spacy.io/usage/spacy-101) \n",
    "- [Introduction to NLP with spaCy](https://towardsdatascience.com/a-short-introduction-to-nlp-in-python-with-spacy-d0aa819af3ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install, go to terminal and run \n",
    "```\n",
    "pip install -U spacy\n",
    "```\n",
    "After installation, also need to download the language model \n",
    "```\n",
    "python -m spacy download en_core_web_lg\n",
    "```\n",
    "\n",
    "To use spacy with English:\n",
    "```\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "```\n",
    "Make sure you install in terminal first before trying to install in this jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install spacy for jupyter notebook.\n",
    "try:\n",
    "    from pip import main as pipmain\n",
    "except:\n",
    "    from pip._internal import main as pipmain\n",
    "packages = ['spacy']\n",
    "pipmain(['install'] + packages);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "- split text into words, symbols, punctuation a.k.a. tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The hungry, hungry catepillar ate all of the food, and then he became a butterfly!\")\n",
    "doc.text.split() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the punctuation gets attached to the previous word. We don't want that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.orth_ for token in doc] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove punctuation by using `.is_punct`   \n",
    "remove spaces by using: `.is_space`   \n",
    "remove stop words by using the `.is_stop`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.orth_ for token in doc if not token.is_punct | token.is_space | token.is_stop] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how all the punctuation, white spaces, and stop words have been removed and we are left only with the \"important\" words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "- reducing a word to its base form or root form\n",
    "- reduce various wordforms to its citation form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use spacy's `.lemma_` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_words = \"going gone went goes\" \n",
    "nlp_lemma_words = nlp(lemma_words) \n",
    "[word.lemma_ for word in nlp_lemma_words] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is especially useful for text classification because lemmatising the text helps avoids word duplication for building models like bag of words model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts-of-speech (POS) Tagging\n",
    "- assign the to words \n",
    "- spacy uses [Penn Treebank POS tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the `.pos_` and `.tag_` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(\"My dog's toy actually belongs to the neighbor's cat.\") \n",
    "pos_tags = [(i, i.tag_) for i in doc2]\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a list of owner-possesion tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i[0].nbor(-1), i[0].nbor(+1)) for i in pos_tags if i[1] == \"POS\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors\n",
    "- the concept of word embeddings is that every word can be represented as a set of real numbers (vectors) that capture the word meaning and context\n",
    "- each word has a unique embedding\n",
    "- word embeddings are multidimensional\n",
    "- similar words have similar embedding values\n",
    "\n",
    "Resources:\n",
    "- [spacy.io: Word Vectors and Semantic Similarity](https://spacy.io/usage/vectors-similarity)\n",
    "- [Get Busy With Word Embeddings](https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/)\n",
    "- [Word Embeddings in Python with Spacy and Gensim](https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy provides pre-trained models for word embeddings which downloaded when we downloaded the English model. Spacy can parse entire blocks of text and assigns word vectors using the loaded model. Then, use `.vector` to get the word vector. \n",
    "\n",
    "Important Note: spaCy's small models (models that end in `sm`) don't ship with word vectors. You can still use `.similarity` to compares, but the results won't be as good. To use real word vectors, make sure to download the large models:\n",
    "```\n",
    "python -m spacy download en_core_web_lg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u\"cat dog water cloud\")\n",
    "print(tokens[0].text, tokens[0].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the word vectors we got from spacy to compare the similarity of the words using `.similarity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Resources\n",
    "- [PythonForLinguistsTalk Gitlab](https://gitlab.com/andersonh/PythonForLinguistsTalk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
